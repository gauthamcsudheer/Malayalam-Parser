{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zy_ZIFKbok7e",
        "outputId": "e901f826-a81a-4f2e-f13f-c6c5e2d3a63f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hZ4XJPn7ou3A",
        "outputId": "8cb86515-149e-4521-9a65-a3ae76e616f4"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/words/adjectives.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ad06e1399aa9>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ad06e1399aa9>\u001b[0m in \u001b[0;36mload_words\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load and preprocess datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/words/adjectives.txt'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, TimeDistributed, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load and preprocess the datasets\n",
        "def load_words(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        words = file.read().splitlines()\n",
        "    return words\n",
        "\n",
        "base_path = '/content/words/'\n",
        "file_names = [\n",
        "    'adjectives.txt', 'adverbs.txt', 'conjunctions.txt', 'interjections.txt', 'nouns.txt', 'person-names.txt',\n",
        "    'places.txt', 'pronouns.txt', 'proper-nouns.txt', 'quantifiers.txt', 'questions.txt', 'times.txt', 'verbs.txt'\n",
        "]\n",
        "\n",
        "# Map file names to their respective POS tags\n",
        "pos_tags = {\n",
        "    'adjectives.txt': 'ADJ', 'adverbs.txt': 'ADV', 'conjunctions.txt': 'CONJ', 'interjections.txt': 'INTERJ',\n",
        "    'nouns.txt': 'NOUN', 'person-names.txt': 'P_NAME', 'places.txt': 'PLACE', 'pronouns.txt': 'PRON',\n",
        "    'proper-nouns.txt': 'P_NOUN', 'quantifiers.txt': 'QUANT', 'questions.txt': 'QUES', 'times.txt': 'TIME',\n",
        "    'verbs.txt': 'VERB'\n",
        "}\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for file_name in file_names:\n",
        "    words = load_words(os.path.join(base_path, file_name))\n",
        "    data.extend(words)\n",
        "    labels.extend([pos_tags[file_name]] * len(words))\n",
        "\n",
        "# Prepare tokenizer\n",
        "tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(data)\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Encode labels\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(labels)\n",
        "label_sequences = tag_tokenizer.texts_to_sequences(labels)\n",
        "y = pad_sequences(label_sequences, maxlen=max_length, padding='post')\n",
        "y = to_categorical(y, num_classes=len(tag_tokenizer.word_index) + 1)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle='TRUE')\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "tag_size = len(tag_tokenizer.word_index) + 1\n",
        "embedding_dim = 64\n",
        "\n",
        "# Step 2: Build the BiLSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
        "    Bidirectional(LSTM(units=64, return_sequences=True)),\n",
        "    TimeDistributed(Dense(units=tag_size, activation='softmax'))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Step 3: Train the Model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 4: Rule-based tagging function\n",
        "suffix_rules = {\n",
        "    'ADJ': ['ം', 'ത്', 'ന്റെ', 'യ', 'യാം'],\n",
        "    'ADV': ['വെ', 'യ', 'തെ', 'തിന്നു'],\n",
        "    'CONJ': ['മറ്റു', 'ഉം', 'എന്നും', 'എന്നാൽ', 'എങ്കിലും'],\n",
        "    'INTERJ': ['അച്ഛാ', 'അയ്യാ'],\n",
        "    'NOUN': ['ന്', 'ം', 'ൾ', 'ൻ', 'ടെ', 'മാര', 'കൾ', 'ത്തിൽ', 'ത്തെ', 'ത്തിൻ', 'ത്തിനും'],\n",
        "    'P_NAME': ['കുമാർ', 'കൃഷ്ണ'],\n",
        "    'PLACE': ['പുരം', 'നാട്'],\n",
        "    'PRON': ['ഞാൻ', 'നീ', 'അവൻ', 'അവൾ', 'അത്', 'അവര', 'താൻ', 'ആരും', 'ഒന്നും'],\n",
        "    'P_NOUN': ['ദില്ലി', 'കൊച്ചി'],\n",
        "    'QUANT': ['ഒന്ന്', 'രണ്ട്', 'മൂന്ന്', 'ആറാമത്'],\n",
        "    'QUES': ['എന്ത്', 'എവിടെ', 'എങ്ങനെ', 'എന്തുകൊണ്ട്'],\n",
        "    'TIME': ['കാലത്ത്', 'വൈകുന്നേരം'],\n",
        "    'VERB': ['ക്ക', 'ന്നു', 'ുന്നു', 'ത', 'ക്കു', 'ട്ടു', 'യാണ്', 'യ്ക്ക', 'ത്തി', 'ച്ച്', 'തും', 'ിച്ചു', 'ും']\n",
        "}\n",
        "\n",
        "def rule_based_tag(word):\n",
        "    for pos, suffixes in suffix_rules.items():\n",
        "        if any(word.endswith(suffix) for suffix in suffixes):\n",
        "            return pos\n",
        "    return None\n",
        "\n",
        "# Step 5: Hybrid POS Tagging Function\n",
        "def hybrid_pos_tag(word):\n",
        "    rule_tag = rule_based_tag(word)\n",
        "    if rule_tag:\n",
        "        return rule_tag\n",
        "    else:\n",
        "        sequence = tokenizer.texts_to_sequences([word])\n",
        "        padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "        pred = model.predict(padded_sequence)\n",
        "        ml_tag = tag_tokenizer.index_word[np.argmax(pred[0], axis=-1)[0]]\n",
        "        return ml_tag\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test_argmax = np.argmax(y_test, axis=-1)\n",
        "    y_pred_argmax = np.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # Flatten the arrays to calculate metrics\n",
        "    y_test_flat = y_test_argmax.flatten()\n",
        "    y_pred_flat = y_pred_argmax.flatten()\n",
        "\n",
        "    # Filter out padding tokens (assuming padding token is 0)\n",
        "    mask = y_test_flat != 0\n",
        "    y_test_filtered = y_test_flat[mask]\n",
        "    y_pred_filtered = y_pred_flat[mask]\n",
        "\n",
        "    accuracy = accuracy_score(y_test_filtered, y_pred_filtered)\n",
        "    precision = precision_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "    recall = recall_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "    f1 = f1_score(y_test_filtered, y_pred_filtered, average='macro')\n",
        "\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1 Score: {f1}')\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(y_test_filtered, y_pred_filtered)\n",
        "\n",
        "    # Get the list of all classes\n",
        "    all_classes = list(tag_tokenizer.index_word.values())\n",
        "    all_classes.insert(0, 'PAD')  # Insert 'PAD' at the beginning for padding class\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=all_classes, yticklabels=all_classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "evaluate_model(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeleSBTRDmom",
        "outputId": "c4bd30e4-67b9-4228-8ba7-21ba9a56596b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "[('മേരി', 'p'), ('ജോണിനെ', 'noun'), ('കണ്ടു', 'noun')]\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "sentence = \"മേരി ജോണിനെ കണ്ടു\"\n",
        "words = sentence.split()\n",
        "\n",
        "tagged_sentence = [(word, hybrid_pos_tag(word)) for word in words]\n",
        "print(tagged_sentence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}